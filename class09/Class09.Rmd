---
title: 'Class 9: Unsupervised Project'
author: "Fiona Moore"
date: "4/30/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Preparing the Data 

```{r}
fn.data <- "class09.csv"
wisc.df <- read.csv("class09.csv")

```

```{r}
head(wisc.df)
```

```{r}
wisc.data <- as.matrix(wisc.df[,3:32])
# Set the row names of wisc.data
row.names(wisc.data) <- wisc.df$id

# Create diagnosis vector by completing the missing code
diagnosis <- as.numeric(wisc.df$diagnosis == "M")
```


How many variables/features in the data are suffixed with _mean?
```{r}
colnames(wisc.data)
```

Grep function will find patterns ("_mean") in a data set.
```{r}
grep("_mean", colnames(wisc.data))
```

```{r}
length(grep("_mean", colnames(wisc.data)))
```


## PCA

```{r}
# Check column means and standard deviations
colMeans(wisc.data)

round(apply(wisc.data,2,sd))
```

We need to scale the data because there is alot of difference bewteen the values. 
```{r}
# Perform PCA on wisc.data
wisc.pr <- prcomp(wisc.data, scale = TRUE)
summary(wisc.pr)
```

Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?
- 44.3%

Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?
- PC3

Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?
- PC7


## Interpreting PCA Results
Trying different plots.

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2], col= diagnosis + 1)
```


```{r}
# Repeat for components 1 and 3
plot(wisc.pr$x[,1], wisc.pr$x[,3], col= diagnosis + 1)
```

```{r}
plot(wisc.pr$x[, 1:3], col = (diagnosis + 1), 
     xlab = "PC1", ylab = "PC3")
```

Because principal component 2 explains more variance in the original data than principal component 3, you can see that the first plot has a cleaner cut separating the two subgroups.

```{r}
biplot(wisc.pr)
```

## Variance Explained 

Making scree plots. 

```{r}
# Calculate variance of each component
pr.var <- wisc.pr$sdev^2 
head(pr.var)
```

```{r}
# Variance explained by each principal component: pve
pve <- (pr.var/ sum(pr.var))*100

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 100), type = "o")

```

```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

```{r}
# Plot cumulative proportion of variance explained
plot(cumsum(pve), xlab = "Principal Component", 
     ylab = "Cumulative Proportion of Variance Explained", 
     ylim = c(0, 100), type = "o")
```

```{r}
# Plot the two graphs together

par(mfcol=c(1,2))

plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 100), type = "o")
plot(cumsum(pve), xlab = "Principal Component", 
     ylab = "Cumulative Proportion of Variance Explained", 
     ylim = c(0, 100), type = "o")


```

## Hierarchical clustering

```{r}
# Scale the wisc.data data: data.scaled
data.scaled <- scale(wisc.data)
```

```{r}
data.dist <- dist(data.scaled)
```

```{r}
wisc.hclust <- hclust(data.dist,method = "complete")
```

Let’s use the hierarchical clustering model you just created to determine a height (or distance between clusters) where a certain number of clusters exists.

```{r}
plot(wisc.hclust)

```

```{r}
plot(wisc.hclust)
abline(h = 19, col="red", lty=2)
```

Selecting numbers for clusters.

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k =4)

table(wisc.hclust.clusters, diagnosis)
```

Q12. Can you find a better cluster vs diagnoses match with by cutting into a different number of clusters between 2 and 10?

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k =2)

table(wisc.hclust.clusters, diagnosis)
```

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k =10)

table(wisc.hclust.clusters, diagnosis)
```

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k =7)

table(wisc.hclust.clusters, diagnosis)
```

## Combining Methods

Using the minimum number of principal components required to describe at least 90% of the variability in the data, create a hierarchical clustering model with the linkage method="ward.D2". We use Ward’s criterion here because it is based on multidimensional variance like principal components analysis. Assign the results to wisc.pr.hclust.


```{r}
wisc.pr.hclust <- hclust(dist(wisc.pr$x[,1:7]), method = "ward.D2")
plot(wisc.pr.hclust)
```

```{r}
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
```

```{r}
table(grps, diagnosis)
```

Graph
```{r}
plot(wisc.pr$x[,1:2], col=grps)
```

```{r}
plot(wisc.pr$x[,1:2], col=diagnosis+1)
```

## Prediction 

We will use the predict() function that will take our PCA model from before and new cancer cell data and project that data onto our PCA space.

```{r}
#url <- "new_samples.csv"
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```

```{r}
plot(wisc.pr$x[,1:2], col=grps)
points(npc[,1], npc[,2], col="blue", pch=16)
```

Q17. Which of these new patients should we prioritize for follow up based on your results?
- We would prioritize the patient witht the data point in the "black" group.
